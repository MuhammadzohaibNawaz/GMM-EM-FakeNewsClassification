import pandas as pd
from nltk.corpus import stopwords
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA

corpus = []
df = pd.read_csv("/content/sample_data/Final.csv")
data=df['text']

# remove special chars and numbers
# remove stopwords
for i in data:
  i = re.sub("[^A-Za-z]+", " ", str(i))
  tokens = nltk.word_tokenize(i)
  tokens = [w for w in tokens if not w.lower() in stopwords.words("english")]
  text = " ".join(tokens)
  text = text.lower().strip()
  corpus.append(text)

#Vectorize the data
vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)
Y = vectorizer.fit_transform(corpus)

X=Y


#apply PCA

pca = PCA(n_components=2, random_state=42)
pca_vecs = pca.fit_transform(X.toarray())
X=pca_vecs
X = (X - X.min()) * (10 - 1) / (X.max() - X.min()) + 1

#Draw the graph
import matplotlib.pyplot as plt

from sklearn.decomposition import PCA
import seaborn as sns
from sklearn.mixture import GaussianMixture
fig = plt.figure()
ax = fig.gca()
ax.scatter(X[:,0], X[:,1], s=3, alpha=0.4)
plt.xlabel('Word')
plt.ylabel('TF-IDF Value')
plt.title('TF-IDF Values for Documents')
ax.autoscale(enable=True) 

from sklearn.cluster import KMeans
import numpy as np
data = X
inertias = []

for i in range(1,11):
  kmeans = KMeans(n_clusters=i)
  kmeans.fit(data)
  inertias.append(kmeans.inertia_)

plt.plot(range(1,11), inertias, marker='o')
plt.title('Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

# Fit a model for a range of values for k
silhouette_scores = []
for k in range(2,11):
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(X)
    silhouette_scores.append(metrics.silhouette_score(X, kmeans.labels_))

# Plot the silhouette scores
plt.plot(range(2,11), silhouette_scores)
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')
plt.show()

import numpy as np
from scipy import random
from matplotlib.patches import Ellipse
import matplotlib.transforms as transforms
from scipy.stats import multivariate_normal

class GMM():
    def __init__(self, k, init_mu=None, init_sigma=None, init_pi=None, colors=None):
        '''
        Define a model with known number of clusters and dimensions.
        input:
            - k: Number of Gaussian clusters
            - init_mu: initial value of mean of clusters (k, dim)
                       (default) random from uniform[-10, 10]
            - init_sigma: initial value of covariance matrix of clusters (k, dim, dim)
                          (default) Identity matrix for each cluster
            - init_pi: initial value of cluster weights (k,)
                       (default) equal value to all cluster i.e. 1/k
            - colors: Color valu for plotting each cluster (k, 3)
                      (default) random from uniform[0, 1]
        '''
        dim=2 #since I have two dimensional data
        self.k = k
        self.dim = dim 
        if(init_mu is None):
            init_mu = random.rand(k, dim)*5 + 2
        self.mu = init_mu
        if(init_sigma is None):
            init_sigma = np.zeros((k, dim, dim))
            for i in range(k):
                init_sigma[i] = np.eye(dim)
        self.sigma = init_sigma
        if(init_pi is None):
            init_pi = np.ones(self.k)/self.k
        self.pi = init_pi
        if(colors is None):
            colors = random.rand(k, 3)
        self.colors = colors
    
    def init_em(self, X):
        '''
        Initialization for EM algorithm.
        input:
            - X: data (batch_size, dim)
        '''
        self.data = X
        self.num_points = X.shape[0]
        self.z = np.zeros((self.num_points, self.k))
    
    def e_step(self):
        '''
        E-step of EM algorithm.
        '''
        for i in range(self.k):
            self.z[:, i] = self.pi[i] * multivariate_normal.pdf(self.data, mean=self.mu[i], cov=self.sigma[i])
        self.z /= self.z.sum(axis=1, keepdims=True)
    
    def m_step(self):
        '''
        M-step of EM algorithm.
        '''
        sum_z = self.z.sum(axis=0)
        self.pi = sum_z / self.num_points
        self.mu = np.matmul(self.z.T, self.data)
        self.mu /= sum_z[:, None]
        for i in range(self.k):
            j = np.expand_dims(self.data, axis=1) - self.mu[i]
            s = np.matmul(j.transpose([0, 2, 1]), j)
            self.sigma[i] = np.matmul(s.transpose(1, 2, 0), self.z[:, i] )
            self.sigma[i] /= sum_z[i]
        return self.mu, self.sigma, self.pi
    def log_likelihood(self, X):
        '''
        Compute the log-likelihood of X under current parameters
        input:
            - X: Data (batch_size, dim)
        output:
            - log-likelihood of X: Sum_n Sum_k log(pi_k * N( X_n | mu_k, sigma_k ))
        '''
        ll = []
        for d in X:
            tot = 0
            for i in range(self.k):
                tot += self.pi[i] * multivariate_normal.pdf(d, mean=self.mu[i], cov=self.sigma[i])
            ll.append(np.log(tot))
        return np.sum(ll)
    import numpy as np

    def predict(self, X, means, covariances, weights):
        """
        Predicts the cluster assignments for the data points in X using the given GMM parameters.
        
        Parameters:
        - X: a numpy array of shape (n_samples, n_features) containing the data points to predict
        - means: a numpy array of shape (n_clusters, n_features) containing the cluster means
        - covariances: a numpy array of shape (n_clusters, n_features, n_features) containing the cluster covariances
        - weights: a numpy array of shape (n_clusters,) containing the cluster weights
        
        Returns:
        - predictions: a numpy array of shape (n_samples,) containing the cluster assignments for each data point
        """
        
        # Number of clusters
        n_clusters = means.shape[0]
        
        # Number of data points
        n_samples = X.shape[0]
        
        # Initialize an array to store the probability of each data point belonging to each cluster
        cluster_probs = np.zeros((n_samples, n_clusters))
        
        # Loop over the clusters
        for i in range(n_clusters):
            # Calculate the probability of each data point belonging to the cluster
            cluster_probs[:, i] = weights[i] * multivariate_normal.pdf(X, mean=means[i], cov=covariances[i])
        
        # Normalize the probabilities
        cluster_probs /= cluster_probs.sum(axis=1, keepdims=True)
        
        # Calculate the cluster assignments for each data point
        predictions = np.argmax(cluster_probs, axis=1)
        
        return predictions

    def draw(self, ax, n_std=2.0, facecolor='none', **kwargs):
        '''
        Function to draw the Gaussians.
        Note: Only for two-dimensionl dataset
        '''
        if(self.dim != 2):
            print("Drawing available only for 2D case.")
            return
        for i in range(self.k):
            self.plot_gaussian(self.mu[i], self.sigma[i], ax, n_std=n_std, edgecolor=self.colors[i], **kwargs)
    def plot_gaussian(self, mean, cov, ax, n_std=3.0, facecolor='none', **kwargs):
        '''
        Utility function to plot one Gaussian from mean and covariance.
        '''
        pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])
        ell_radius_x = np.sqrt(1 + pearson)
        ell_radius_y = np.sqrt(1 - pearson)
        ellipse = Ellipse((0, 0),
            width=ell_radius_x * 2,
            height=ell_radius_y * 2,
            facecolor=facecolor,
            **kwargs)
        scale_x = np.sqrt(cov[0, 0]) * n_std
        mean_x = mean[0]
        scale_y = np.sqrt(cov[1, 1]) * n_std
        mean_y = mean[1]
        transf = transforms.Affine2D() \
            .rotate_deg(45) \
            .scale(scale_x, scale_y) \
            .translate(mean_x, mean_y)
        ellipse.set_transform(transf + ax.transData)
        return ax.add_patch(ellipse)
        
def plot(title):
    '''
    Draw the data points and the fitted mixture model.
    input:
        - title: title of plot and name with which it will be saved.
    '''
    fig = plt.figure(figsize=(8, 8))
    ax = fig.gca()
    ax.scatter(X[:, 0], X[:, 1], s=3, alpha=0.4)
    ax.scatter(gmm.mu[:, 0], gmm.mu[:, 1], c=gmm.colors)
    gmm.draw(ax, lw=3)
    ax.set_xlim((-1,13))
    ax.set_ylim((-1, 13))
    
    plt.title(title)
    plt.xlabel('Word')
    plt.ylabel('TF-IDF Value')
    plt.show()
    plt.clf()
    
gmm = GMM(k=2)
# Training the GMM using EM
allmu=[]
allsig=[]
allpi=[]
# Initialize EM algo with data
gmm.init_em(X)
num_iters = 30
# Saving log-likelihood
log_likelihood = [gmm.log_likelihood(X)]
# plotting
plot("Iteration:  0")
for e in range(num_iters):
    # E-step
    gmm.e_step()
    # M-step
    mu,sig,pi=gmm.m_step()
    allmu.append(mu)
    allsig.append(sig)
    allpi.append(pi)
    # Computing log-likelihood
    log_likelihood.append(gmm.log_likelihood(X))
    print("Iteration: {}, log-likelihood: {:.4f}".format(e+1, log_likelihood[-1]))
    # plotting
    plot(title="Iteration: " + str(e+1))
    
    
    from sklearn.cluster import AgglomerativeClustering
agg = AgglomerativeClustering(n_clusters=2)

# Fit the model to the data
agg.fit(X)

# Get the cluster labels
labels = agg.labels_

# Plot the data, coloring the points by cluster label
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.title("Result of Agglomerative clustering")
plt.xlabel("Wrods")
plt.ylabel("TFIDF")

# Show the plot
plt.show()

plt.plot(log_likelihood[1:], marker='.')

for i in range(1, len(log_likelihood)):
    plt.title("likelihood for iteration: " + str(i))
    plt.xlabel("Iterations")
    plt.ylabel("Likelihood")
    plt.plot(log_likelihood[1:1+i], marker='.')
    axes = plt.axes()
    axes.set_ylim([min(log_likelihood[1:])-50, max(log_likelihood[1:])+50])
    axes.set_xlim([-2, 32])
    
muList=[]
print(type(allmu))
for i in allmu:
  muList.append(i[0])

x = [row[0] for row in muList]
y=[]
for i in range(1,31):
  y.append(i)
# x.reverse()

# Plot the x and y values
plt.plot(x, y)

# Add a title and labels
plt.title("mu vs iteration")
plt.xlabel("mu")
plt.ylabel("Iteration")

# Show the plot
plt.show()



import numpy as np

# Generate random data
data = np.random.rand(10, 2)

# Create the array
XNew = np.array(data)
XNew = (XNew - XNew.min()) * (10 - 1) / (XNew.max() - XNew.min()) + 1
# print(X)  # (10, 2)
nmu=allmu[-1]
npi=allpi[-1]
nsig=allsig[-1]


predictions = gmm.predict(XNew, nmu, nsig, npi)


# Create a scatter plot of the original data
plt.scatter(X[:, 0], X[:, 1], c='blue', marker='o')

# Add the new data to the plot, coloring it differently
plt.scatter(XNew[:, 0], XNew[:, 1], c='red', marker='x')
plt.title("New data points with old point")
plt.xlabel('Word')
plt.ylabel('TF-IDF Value')
# Show the plot
plt.show()



# Create a scatter plot of the original data
plt.scatter(X[:, 0], X[:, 1], c='pink', marker='o')

# Add the new data to the plot, coloring it according to the predicted cluster assignments
plt.scatter(XNew[:, 0], XNew[:, 1], c=predictions, marker='x', cmap='viridis')

# # Plot the cluster contours
plt.title('Prediction of new points with old points')
plt.xlabel('Word')
plt.ylabel('TF-IDF Value')
plt.show()
